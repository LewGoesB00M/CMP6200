{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to access the saved Chroma vector database. \n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Used to embed the user's query with text-embedding-3-small \n",
    "# and send it to gpt-4o-mini.\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Used to get the OpenAI API key from environment variables so that \n",
    "# it's not visible in this code.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the API key.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# I chose not to use Pinecone, as it adds considerable complication to the code.\n",
    "# os.environ[\"PINECONE_API_KEY\"] = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Sets the directory of the Chroma DB that's being loaded from.\n",
    "# At present, it's just \"Chroma\", but I may make seperate DBs for different embedding models.\n",
    "CHROMA_PATH = \"Chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step of engineering the prompt. The LLM is told to answer the question with \n",
    "# the given context. Currently, there isn't any. However, this will be formatted later\n",
    "# to take this string and add the context of the relevant database chunk in place of {context}\n",
    "# and add the user's query in place of {question}.\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the embedding model with the API key.\n",
    "# Conveniently, LangChain allows for \"hot-swapping\" of embedding models by merely\n",
    "# changing the model argument. However, using a different embedding model than the one \n",
    "# used to create the vector database will have significant negative consequences that could \n",
    "# render the chatbot inoperable, so it's essential that this matches what's used in Chroma.py.\n",
    "embedder = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector database.\n",
    "db = Chroma(persist_directory = CHROMA_PATH, embedding_function = embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY, THIS IS THE ONLY WAY OF QUERYING THE DB AND LLM.\n",
    "# I'm looking into a package called \"Streamlit\", which provides a web interface for user input.\n",
    "# Alternatively, I think it'd be good to somehow get this as a bot on a messaging service (Teams, Discord)\n",
    "# and allow queries to flow in from there. (I have yet to do any research on Teams or Discord LLM bots.)\n",
    "query = \"What university is this? How do they protect my data? What are their disability support arrangements?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40908483090004344\n",
      "0.40572125950778304\n",
      "0.39656313766384355\n",
      "\n",
      "Answer the question based only on the following context:\n",
      "\n",
      "13\n",
      "Terms and conditions for taught and research students\n",
      "y. The University’s Safeguarding Policy sets out the University’s serious \n",
      "commitment to safeguarding matters. Safeguarding is everyone’s \n",
      "responsibility, and any safeguarding concerns should be raised under the \n",
      "policy. The University will take any necessary action to fulfil any safeguarding \n",
      "responsibilities it has.\n",
      "DISABILITY SUPPORT AND  \n",
      "REASONABLE ADJUSTMENTS\n",
      "21. The University is committed to providing an inclusive and accessible\n",
      "\n",
      "---\n",
      "\n",
      "studies in line with the University’s Data Protection Policy. The University’s \n",
      "Privacy Notices which explains what happens to any personal data you \n",
      "provide to us or that we collect from you (including your data subject rights) \n",
      "and the University’s Graduation Ceremonies Privacy Notice which concerns \n",
      "the live streaming of graduation ceremonies at the University.\n",
      "t. The University’s rules regarding the use of University computer systems and\n",
      "\n",
      "---\n",
      "\n",
      "23\n",
      "Terms and conditions for taught and research students\n",
      "DATA PROTECTION\n",
      "52. As part of your application process the University told you how the University \n",
      "would use your personal data (being any information which relates to or \n",
      "identifies you personally as an individual) including your sensitive personal data \n",
      "(which is called special category data in the law), in order for the University \n",
      "to process your application and for related purposes including compliance\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What university is this? How do they protect my data? What are their disability support arrangements?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finds the three most likely chunks that the user's query applies to.\n",
    "results = db.similarity_search_with_relevance_scores(query, k=3)\n",
    "\n",
    "### This sorts them in the wrong order by default, with the best chunk coming last.\n",
    "### I'm not sure how to change that yet.\n",
    "for doc, _score in results:\n",
    "    print(_score)\n",
    "\n",
    "# Creates the {context} previously seen in the PROMPT_TEMPLATE.\n",
    "# Seperates the three most likely chunks with new lines and dashes.\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "# Uses LangChain's ChatPromptTemplate as previously mentioned to engineer a prompt\n",
    "# to pass to the LLM.\n",
    "# prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "# Formats the prompt to put the document context in place of {context},\n",
    "# and the user query in place of {question}\n",
    "prompt = PROMPT_TEMPLATE.format(context=context_text, question=query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The context provided does not specify the name of the university. However, it indicates that the university is committed to safeguarding matters, providing an inclusive and accessible environment for students, and has a Data Protection Policy in place. \n",
      "\n",
      "To protect your data, the university informs you about how your personal data, including sensitive personal data, will be used during the application process and for related purposes, ensuring compliance with data protection laws. They also have Privacy Notices that explain the handling of personal data and your rights regarding it.\n",
      "\n",
      "Regarding disability support arrangements, the university is committed to providing inclusive and accessible studies, which suggests that they have measures in place to support students with disabilities, although specific details are not provided in the context.\n",
      "Sources: ['Data\\\\Policies\\\\StudentContract.pdf', 'Data\\\\Policies\\\\StudentContract.pdf', 'Data\\\\Policies\\\\StudentContract.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM object.\n",
    "## Uses GPT-4o-mini for cost efficiency.\n",
    "## Temperature can be changed to vary the LLMs responses.\n",
    "## Loads the API key from the system environment variables.\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0,\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Gets the LLM's response.\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "# Attaches the document that the relevant chunks were found from.\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\n",
    "# Saves and outputs the LLM's response, and the sources used to generate it.\n",
    "formatted_response = f\"Response: {response}\\nSources: {sources}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMP6200",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
