{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to access the saved FAISS vector database. \n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Used to embed the user's query with text-embedding-3-small \n",
    "# and send it to gpt-4o-mini.\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Used to get the statistics of each prompt, notably the input tokens,\n",
    "# output tokens, and associated cost.\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "# Used to get the OpenAI API key from environment variables so that \n",
    "# it's not visible in this code.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the API key.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# I chose not to use Pinecone, as it adds considerable complication to the code.\n",
    "# os.environ[\"PINECONE_API_KEY\"] = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Sets the directory of the FAISS DB that's being loaded from.\n",
    "# At present, it's just \"FAISS\", but I may make seperate DBs for different embedding models.\n",
    "FAISS_PATH = \"FAISS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step of engineering the prompt. The LLM is told to answer the question with \n",
    "# the given context. Currently, there isn't any. However, this will be formatted later\n",
    "# to take this string and add the context of the relevant database chunk in place of {context}\n",
    "# and add the user's query in place of {question}.\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the embedding model with the API key.\n",
    "# Conveniently, LangChain allows for \"hot-swapping\" of embedding models by merely\n",
    "# changing the model argument. However, using a different embedding model than the one \n",
    "# used to create the vector database will have significant negative consequences that could \n",
    "# render the chatbot inoperable, so it's essential that this matches what's used in Chroma.py.\n",
    "embedder = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector database.\n",
    "db = FAISS.load_local(folder_path = FAISS_PATH, embeddings = embedder,\n",
    "                      allow_dangerous_deserialization=True) # Not sure what this argument is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY, THIS IS THE ONLY WAY OF QUERYING THE DB AND LLM.\n",
    "# I'm looking into a package called \"Streamlit\", which provides a web interface for user input.\n",
    "# Alternatively, I think it'd be good to somehow get this as a bot on a messaging service (Teams, Discord)\n",
    "# and allow queries to flow in from there. (I have yet to do any research on Teams or Discord LLM bots.)\n",
    "query = \"How do I make an EC claim?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48894476375218254\n",
      "0.2985927371375807\n",
      "0.29469642958014264\n"
     ]
    }
   ],
   "source": [
    "# Finds the three most likely chunks that the user's query applies to.\n",
    "results = db.similarity_search_with_relevance_scores(query, k=3)\n",
    "\n",
    "for doc, _score in results:\n",
    "    #print(doc)\n",
    "    print(_score)\n",
    "\n",
    "# Creates the {context} previously seen in the PROMPT_TEMPLATE.\n",
    "# Seperates the three most likely chunks with new lines and dashes.\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "# Formats the prompt to put the document context in place of {context},\n",
    "# and the user query in place of {question}\n",
    "prompt = PROMPT_TEMPLATE.format(context=context_text, question=query)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini says:  \n",
       "        To make an Extenuating Circumstances (EC) claim, follow these steps:\n",
       "\n",
       "1. Log in to your profile on MySRS at mysrs.bcu.ac.uk.\n",
       "2. Click on the ‘Support’ tab.\n",
       "3. Select the ‘Extenuating Circumstances’ option.\n",
       "4. Ensure to include all assessments you wish to claim for in your submission.\n",
       "5. If you encounter any technical issues, contact mysrs@bcu.ac.uk for assistance.\n",
       "6. If you do not have access to MySRS or cannot make a claim electronically, reach out to Student Governance at appealsandresolutions@bcu.ac.uk for an alternative method to submit your claim.\n",
       "\n",
       "Additionally, refer to the separate User Guidance document available from iCity for step-by-step instructions on making a claim through MySRS.  \n",
       "        Sources: ['Data\\\\Policies\\\\ExtenuatingCircumstances.pdf', 'Data\\\\Policies\\\\AssessmentAndFeedback.pdf', 'Data\\\\Policies\\\\ExtenuatingCircumstances.pdf']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 753\n",
      "Prompt Tokens: 585\n",
      "Completion/Output Tokens: 168\n",
      "Total Cost (USD): $0.00018855\n"
     ]
    }
   ],
   "source": [
    "# Used to format the LLM's output response. \n",
    "# Completely unnecessary, just makes this notebook look nicer.\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create the LLM object.\n",
    "## Uses GPT-4o-mini for cost efficiency.\n",
    "## Temperature can be changed to vary the LLMs responses.\n",
    "## Loads the API key from the system environment variables.\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0,\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# To track the stats (input/output tokens, cost) of each prompt,\n",
    "# get the API callback.\n",
    "with get_openai_callback() as cb:\n",
    "    # Gets the LLM's response.\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Attaches the document that the relevant chunks were found from.\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\n",
    "    # Saves and outputs the LLM's response, and the sources used to generate it.\n",
    "    formatted_response = f\"\"\"{llm.model_name} says:  \n",
    "        {response.content}  \n",
    "        Sources: {sources}\"\"\"\n",
    "    display(Markdown(formatted_response))\n",
    "\n",
    "    # Code from LangChain example (https://python.langchain.com/docs/how_to/llm_token_usage_tracking/)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion/Output Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
