{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to access the saved Chroma vector database. \n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Used to embed the user's query with text-embedding-3-small \n",
    "# and send it to gpt-4o-mini.\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Used to get the statistics of each prompt, notably the input tokens,\n",
    "# output tokens, and associated cost.\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "# Used to get the OpenAI API key from environment variables so that \n",
    "# it's not visible in this code.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the API key.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# I chose not to use Pinecone, as it adds considerable complication to the code.\n",
    "# os.environ[\"PINECONE_API_KEY\"] = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Sets the directory of the Chroma DB that's being loaded from.\n",
    "# At present, it's just \"Chroma\", but I may make seperate DBs for different embedding models.\n",
    "CHROMA_PATH = \"Chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step of engineering the prompt. The LLM is told to answer the question with \n",
    "# the given context. Currently, there isn't any. However, this will be formatted later\n",
    "# to take this string and add the context of the relevant database chunk in place of {context}\n",
    "# and add the user's query in place of {question}.\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the embedding model with the API key.\n",
    "# Conveniently, LangChain allows for \"hot-swapping\" of embedding models by merely\n",
    "# changing the model argument. However, using a different embedding model than the one \n",
    "# used to create the vector database will have significant negative consequences that could \n",
    "# render the chatbot inoperable, so it's essential that this matches what's used in Chroma.py.\n",
    "embedder = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lewis\\AppData\\Local\\Temp\\ipykernel_34256\\962685837.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory = CHROMA_PATH, embedding_function = embedder)\n"
     ]
    }
   ],
   "source": [
    "# Load the vector database.\n",
    "db = Chroma(persist_directory = CHROMA_PATH, embedding_function = embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY, THIS IS THE ONLY WAY OF QUERYING THE DB AND LLM.\n",
    "# I'm looking into a package called \"Streamlit\", which provides a web interface for user input.\n",
    "# Alternatively, I think it'd be good to somehow get this as a bot on a messaging service (Teams, Discord)\n",
    "# and allow queries to flow in from there. (I have yet to do any research on Teams or Discord LLM bots.)\n",
    "query = \"What is the university's approach to teaching?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076361850953075\n",
      "0.2226055118480279\n",
      "0.20876995397096276\n",
      "\n",
      "Answer the question based only on the following context:\n",
      "\n",
      "5.\n",
      "\n",
      "Principles\n",
      "\n",
      "1. The University expects students to attend all scheduled learning sessions (on campus and/or online) and to engage with all learning activities, resources and assessments during their studies. Students should be proactive in seeking support if they face any challenges preventing them from engaging with their studies to the expected level.\n",
      "\n",
      "2. This policy is underpinned by the Student Attendance and Engagement Operational Guidance. The University community and our students all have responsibilities in relation to attendance and engagement as detailed in this guidance, section 8.\n",
      "\n",
      "3. Attendance at some scheduled learning sessions (e.g., laboratory sessions, studio sessions, workshops) may be mandatory due to PSRB requirements. The student Course Handbook will make clear where there are any enhanced attendance requirements and any consequences of non-attendance.\n",
      "\n",
      "---\n",
      "\n",
      "6\n",
      "\n",
      "6.\n",
      "\n",
      "How the University will Monitor Attendance and Engagement\n",
      "\n",
      "1. The University wants to support students to maximise their potential and will monitor attendance at scheduled learning sessions from the first week of teaching. There will be a range of informal and formal interventions throughout the academic year and where attendance and/or engagement is flagged as a concern, we will contact a student, initiating formal attendance notifications.\n",
      "\n",
      "---\n",
      "\n",
      "Submission of assessments.\n",
      "\n",
      "Attendance at examinations or time-constrained assessment activities.\n",
      "\n",
      "Engagement with the virtual learning environment (Moodle).\n",
      "\n",
      "Responding in a timely manner to communications from the University.\n",
      "\n",
      "â€¢ Proactive engagement with Course Team and/or Personal Tutor.\n",
      "\n",
      "4\n",
      "\n",
      "4. Scope\n",
      "\n",
      "1. The policy applies to all students, whether on an undergraduate (UG) or postgraduate taught (PGT) course who are required to attend scheduled learning sessions at a BCU campus and/or online in the UK. It also applies to postgraduate research (PGR) students who are required to attend formal supervision sessions and students on a Study Abroad programme, studying at the University or overseas at a host institution. A separate policy applies to apprentices on apprenticeship programmes.\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the university's approach to teaching?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finds the three most likely chunks that the user's query applies to.\n",
    "results = db.similarity_search_with_relevance_scores(query, k=3)\n",
    "\n",
    "for doc, _score in results:\n",
    "    #print(doc)\n",
    "    print(_score)\n",
    "\n",
    "# Creates the {context} previously seen in the PROMPT_TEMPLATE.\n",
    "# Seperates the three most likely chunks with new lines and dashes.\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "# Formats the prompt to put the document context in place of {context},\n",
    "# and the user query in place of {question}\n",
    "prompt = PROMPT_TEMPLATE.format(context=context_text, question=query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini says:  \n",
       "        The university's approach to teaching emphasizes the importance of attendance and engagement in scheduled learning sessions, whether on campus or online. It expects students to actively participate in all learning activities, resources, and assessments, and encourages them to seek support if they encounter challenges. The university monitors attendance from the first week of teaching and implements both informal and formal interventions if attendance or engagement is a concern. Additionally, certain sessions may have mandatory attendance requirements due to professional, statutory, and regulatory body (PSRB) obligations. Overall, the university aims to support students in maximizing their potential through proactive engagement and communication.  \n",
       "        Sources: ['Data\\\\Policies\\\\TESTING\\\\Attendance.pdf', 'Data\\\\Policies\\\\TESTING\\\\Attendance.pdf', 'Data\\\\Policies\\\\TESTING\\\\Attendance.pdf']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 538\n",
      "Prompt Tokens: 419\n",
      "Completion/Output Tokens: 119\n",
      "Total Cost (USD): $0.00013424999999999998\n"
     ]
    }
   ],
   "source": [
    "# Used to format the LLM's output response. \n",
    "# Completely unnecessary, just makes this notebook look nicer.\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create the LLM object.\n",
    "## Uses GPT-4o-mini for cost efficiency.\n",
    "## Temperature can be changed to vary the LLMs responses.\n",
    "## Loads the API key from the system environment variables.\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0,\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# To track the stats (input/output tokens, cost) of each prompt,\n",
    "# get the API callback.\n",
    "with get_openai_callback() as cb:\n",
    "    # Gets the LLM's response.\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Attaches the document that the relevant chunks were found from.\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\n",
    "    # Saves and outputs the LLM's response, and the sources used to generate it.\n",
    "    formatted_response = f\"\"\"{llm.model_name} says:  \n",
    "        {response.content}  \n",
    "        Sources: {sources}\"\"\"\n",
    "    display(Markdown(formatted_response))\n",
    "\n",
    "    # Code from LangChain example (https://python.langchain.com/docs/how_to/llm_token_usage_tracking/)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion/Output Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "    \n",
    "### CURRENTLY, THIS IS QUERYING A DB GENERATED FROM POLICIES/TESTING, AS CHROMA DOESN'T WORK WITH MANY DOCUMENTS?\n",
    "### Likely alleviated with PyPDFLoader as used before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
