The longer a conversation goes on, the amount of tokens per message will greatly increase because the previous messages are still sent as input.
Experimentation with Pinecone indicates that it should be nearly impossible to hit the monthly limits. (SHOULD)

To avoid the potential risk of Pinecone's limits altogether, the relatively small size of the documents I'm embedding can be stored
in a FAISS (Facebook AI Similarity Search) database. This is stored locally, and is half the size of a Chroma database with no 
observed performance deficit. My desktop is relatively powerful, and while FAISS-GPU is restricted to only Linux distros,
FAISS-CPU works on Windows and has been suitable for this project so far.

The package manager for this project was originally Conda, though it became hideously slow as packages were added. For this reason
I searched and found "UV", an alternative package manager that worked over 30x faster for setting up the venv. This also means 
the artifact no longer requires a Conda environment which could perhaps make it more distributable?

UV commands:
    uv add PACKAGE - Downloads the package & dependencies. Also adds it to pyproject.toml.
    uv pip freeze > requirements.txt - Not necessary due to pyproject.toml but the option is there. Exports package list.
    uv sync - Reads pyproject.toml and installs all dependencies. Creates a venv if one doesn't already exist.