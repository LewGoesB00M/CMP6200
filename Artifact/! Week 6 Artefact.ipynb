{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Artificially Intelligent Chatbot\n",
    "## Week 3 Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current checklist:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checklist isn't exhaustive, more things may be added to it as time passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Data mining*\n",
    "  - üõ†Ô∏è Policies\n",
    "  - ‚ùå Campus locations\n",
    "  - ‚ùå Societies\n",
    "- *Data Storage*\n",
    "  - ‚úÖ Vector DB Identification\n",
    "  - ‚úÖ Embedding\n",
    "    - Using OpenAI text-embedding-3-small\n",
    "- *Chatbot*  \n",
    "  - ‚úÖ Data retrieval\n",
    "  - ‚úÖ Conversational memory\n",
    "  - ‚ùå User interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future plans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt engineering \n",
    "    - The prototype seems to work okay with its current prompt, but maybe it could be even better.\n",
    "    \n",
    "- More policy data\n",
    "    - A small selection of University policies can currently be queried, though not all of them.\n",
    "\n",
    "\n",
    "- Tools \n",
    "    - Currently, the LLM can use one tool (if it deems it necessary), which is to retrieve data from the DB.\n",
    "            Perhaps more tools can be added for general use, like telling the time? (How long until this deadline, etc)\n",
    "\n",
    "- Manual Data Creation\n",
    "    - While the LLM can gather some general information about the university from the policies it retrieves, it will be helpful (perhaps essential) to create an additional PDF of my own with some information about the university, such as the campus locations. This may also be the best way to provide data about societies? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and key variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported methods and classes are described in further detail when they're used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to get the OpenAI API key from the system environment variables.\n",
    "import os\n",
    "\n",
    "# Initialises the LLM.\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# The vector DB used for this prototype. It's worked well this far,\n",
    "# so I'll likely use it in future versions, too.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Allows for inputs to be sent to the LLM as a human (user input)\n",
    "# and the system (specialised prompt that defines LLM behaviour).\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Allows for \"tools\" to be created. The LLM can use these tools\n",
    "# to execute defined code, such as retrieving data from the FAISS DB.\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Used to embed queries to the FAISS DB.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Used to save the conversation to memory.\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangGraph key functionality. Declares a clear and consistent \n",
    "# structure for the chatbot. LangGraph is described in detail in \n",
    "# its own notebook section.\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Get the OpenAI API key from environment variables so that \n",
    "# it's not visible in this code on GitHub. OpenAI would revoke the key if it leaked.\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Sets the directory of the FAISS DB that's being loaded from.\n",
    "    # Options:\n",
    "    #   FAISS: Chunk size 1000, Overlap 200, UnstructuredPDFLoader in elements mode.\n",
    "    #   FAISS-PyPDF: Chunk size 1000, Overlap 200, PyPDFLoader with default args.\n",
    "FAISS_PATH = \"FAISS-PyPDF\"\n",
    "# Experimentation showed that using the PyPDFLoader-based DB gave better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS, embeddings and the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain allows for easy switching of embedding models by merely changing the model argument. However, using a different embedding model than the one used to create the vector database will have significant negative consequences that could render the chatbot inoperable, so it's essential that this matches what's used in the database embedding file, which is OpenAI's ``text-embedding-3-small`` in this prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the embedding model with the API key.\n",
    "embedder = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also allows for many different vector database options. This prototype uses a Facebook AI Similarity Search (FAISS) DB primarily due to its easy integration with LangChain - this single line of code is all that's necessary to retrieve the stored data from the chosen ``FAISS_PATH``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector database.\n",
    "db = FAISS.load_local(folder_path = FAISS_PATH,\n",
    "                      embeddings = embedder,\n",
    "                      allow_dangerous_deserialization=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS stores data in a Pickle file. This is a serialised format that allows Python to load the database. However, a malicious Pickle file can actually execute arbitrary code. The files used in this vector DB are not malicious, so it is fine to enable ``allow_dangerous_deserialization``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``init_chat_model``, as suggested by its name, initialises the model. This prototype, and most likely the final version, will use GPT-4o-mini due to its low cost in relation to other models. While it is a lower-quality model than higher-end models like GPT-4 or reasoning models like o1/o3-mini, it still can perform the simple chatbot functionalities of this prototype. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there's only one tool - the retriever itself, which will perform a semantic search on the FAISS DB based on the user's query. It returns the content of the 3 most similar chunks to the user's query, as well as which PDF they came from, though the user won't see that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format = \"content_and_artifact\")\n",
    "def retrieve(query):\n",
    "    # The docstring below is actually REQUIRED by LangGraph, and this \n",
    "    # won't run without it.\n",
    "    \"\"\"Retrieves the 3 most relevant chunks for the user's query.\"\"\"\n",
    "    retrieved_docs = db.similarity_search(query, k = 3)\n",
    "    \n",
    "    # The chunk's content and the document it came from (e.g \"Attendance.pdf\")\n",
    "    content = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return content, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph is a new option from the LangChain devs, which allow for the actions in the RAG chain to be directly plotted as a sequence of events as part of a directed graph.\n",
    "A key benefit is that it makes **conversational memory** extremely simple to implement. Many companies use LangGraph according to [their website](https://www.langchain.com/langgraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise an empty graph. Nodes and edges are added later.\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ``MessagesState`` is a list of messages, which contains the active conversation. It's **append-only**, meaning that it can't be accidentally overwritten and will always keep the conversation in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_or_respond(state: MessagesState):\n",
    "    # Allows the LLM to use the retrieve tool that was created earlier.\n",
    "    rag_llm = llm.bind_tools([retrieve])\n",
    "    \n",
    "    # The LLM decides on its own if it needs\n",
    "    response = rag_llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    # A key element of using a MessagesState is that that will append\n",
    "    # the response to the conversation history rather than overwriting.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Creating a node for the list of tools that the LLM can access. \n",
    "# At the moment, it's only one tool, the retriever, but other tools can \n",
    "# easily be added later by appending them to this list.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# The final step of the process, generating a message based on the info gathered\n",
    "# from the retrieval tool.\n",
    "def generate(state: MessagesState):\n",
    "    # To massively reduce token consumption (and therefore cost),\n",
    "    # the most recent RAG context from tool calls is added to the \n",
    "    # prompt to stop the LLM searching the entire conversation history \n",
    "    # for something that was JUST said. \n",
    "    recent_tool_messages = []\n",
    "    \n",
    "    # To get the most recent ones, the list needs to be reversed\n",
    "    # so that the most recent come first instead of last.\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            # If it's a normal message, stop.\n",
    "            break\n",
    "    \n",
    "    # Put the tool messages in their original order in case \n",
    "    # the sequence of the retrieved context mattered. \n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Saves the context from the recent tool messages.\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    \n",
    "    # This is the LLM's system prompt, which decides how the LLM behaves.\n",
    "    system_message_content = (\n",
    "        # This is formatted like this to follow the Ruff linter's line length rule.\n",
    "        \"You are an assistant to help new students get acclimated to Birmingham City \"\n",
    "        \"University. Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. When referring to context, be specific and quote the context. \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\" # RAG context is attached here\n",
    "    )\n",
    "    \n",
    "    # The list of messages in the conversation.\n",
    "    # Only adds messages that AREN'T tool calls, as tool calls\n",
    "    # would hugely increase the input tokens used, and the LLM \n",
    "    # should (hopefully) have already said the useful info in its response\n",
    "    # so it can use that instead.\n",
    "    conversation_messages = [\n",
    "        message for message in state[\"messages\"] # Every message in the conversation\n",
    "        if message.type in (\"human\", \"system\")# If it's human input or the system prompt\n",
    "        or (message.type == \"ai\" and not message.tool_calls)# Or AI and not a tool call.\n",
    "    ]\n",
    "    \n",
    "    # The final prompt consists of the system prompt and the conversation.\n",
    "    # This does mean that as a conversation continues, token cost will greatly increase.\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Get the LLM's response to the prompt and return the response.\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After establishing all the functions that form the chatbot, the graph can be built using the graph_builder `StateGraph` that was made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x16552eea2f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add all the nodes.\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``query_or_respond`` has **conditions**:\n",
    "- If the user's query needs RAG context, the retrieve tool will be called.\n",
    "- If it does not, the LLM will generate a response by itself.\n",
    "\n",
    "\"What is the late submission deadline?\" invokes the retrieval tool. \n",
    "\n",
    "\"Hello!\" should not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x16552eea2f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition, \n",
    "    # If retrieval is not needed, generate a general answer.\n",
    "    # If it is, call the retrieval tool.\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "\n",
    "# An edge is also needed between the tool call and response generation\n",
    "# to ensure the response has the RAG context.\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "\n",
    "# After the response is generated, the graph is done.\n",
    "graph_builder.add_edge(\"generate\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about LangGraph and memory, I used their documentation at https://langchain-ai.github.io/langgraph/concepts/persistence/.\n",
    "\n",
    "`MemorySaver` saves the active conversation to the memory (RAM). This does mean that data is lost on the Jupyter kernel restarting. It's referred to as a **checkpointer**, as it will save each step of the chain. For example, if `query_and_respond` and `retrieve` succeed, but `generate` does not, it would be possible to see the current state of the chain before `generate` failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to keep the ongoing conversation in memory.\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Thread ID would allow for multiple sessions of the chatbot to run simultaneously.\n",
    "# Different thread IDs have their own memory.\n",
    "config = {\"configurable\": {\"thread_id\": \"W3Prototyping\"}}\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile(checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query):\n",
    "    input_messages = [HumanMessage(query)]\n",
    "    output = graph.invoke({\"messages\": input_messages}, config)\n",
    "    output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype Main Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running all earlier cells, the cell below contains an infinite loop (broken by inputting \"quit\" or CTRL+C) to prompt the entire RAG chain as dictated by the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice to meet you, Lewis! How can I assist you today?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If your assignment is submitted between 24 hours to one week (5 working days) after the published deadline, your original mark will be reduced by 10%. For instance, if you scored 60%, it would be reduced to 54%. This deduction does not apply if your mark is below 40% (or 50% for postgraduate courses).\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If your assignment is submitted between 1 and 24 hours after the published submission deadline, the original mark awarded will be reduced by 5%. For example, if your original mark was 60%, it would be reduced to 57%. However, if your mark is below 40% (or 50% for postgraduate courses), the reduction will not apply, and you would receive the mark you earned.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    q = input(\"What is your query?\")\n",
    "    if q != \"quit\":\n",
    "        query(q)\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below outputs the entire conversation. It does include tool messages and the context they retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the conversation continues, the token usage will begin to vastly increase. (5 small prompts are approx 1p)** OpenAI automatically caches recently input tokens, and charges a lower rate for them. This means that while the cost will increase, LangGraph's conversational memory saves some money with how it handles the prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is Lewis\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Lewis! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What happens if I submit my assignment 3 days late?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_R9cS08o5At9esIRzgrRClmWD)\n",
      " Call ID: call_R9cS08o5At9esIRzgrRClmWD\n",
      "  Args:\n",
      "    query: late assignment policy\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'Data\\\\Policies\\\\LateAssessment.pdf', 'page': 0, 'page_label': '1', 'start_index': 0}\n",
      "Content: Late Submission of Assessment Policy\n",
      "1\n",
      " 1\n",
      "Effective from September 2024\n",
      "LATE SUBMISSION \n",
      "OF ASSESSMENT \n",
      "POLICY\n",
      "First Edition, June 2022\n",
      "Academic Regulations \n",
      "and Policy Committee\n",
      "\n",
      "Source: {'source': 'Data\\\\Policies\\\\LateAssessment.pdf', 'page': 1, 'page_label': '2', 'start_index': 0}\n",
      "Content: Late Submission of Assessment Policy\n",
      "2\n",
      " 2\n",
      "1. Intr oduction\n",
      "2\n",
      "1 Assessments must be submitted in the format specified in the assessment \n",
      "task, by the deadline and to the submission point published on Moodle. Where \n",
      "a student submits work after the published submission deadline, the mark they \n",
      "receive for the assessment will be reduced according to how long after the \n",
      "deadline they submit.\n",
      "2 1.2 A student may apply for an extension of time to complete assessed \n",
      "coursework if there are personal circumstances, which are unforeseen and \n",
      "unpreventable and have an effect on the student‚Äôs ability to submit work by \n",
      "the published hand-in deadline. Further detail on the process for making an \n",
      "Extenuating Circumstances claim for an extension is set out in the Extenuating \n",
      "Circumstance Procedure . \n",
      "2. P enalties for Late Submission\n",
      "1 Where a student submits a first attempt at an assessment between 1 and 24 \n",
      "hours after the published submission deadline, the original mark awarded will\n",
      "\n",
      "Source: {'source': 'Data\\\\Policies\\\\LateAssessment.pdf', 'page': 2, 'page_label': '3', 'start_index': 0}\n",
      "Content: Late Submission of Assessment Policy\n",
      "3\n",
      " 3\n",
      "Document Control Statement\n",
      "Document Type Late Submission of Assessment Policy\n",
      "Document Owner Head of Academic Standards and Governance\n",
      "Division / Service Quality Assurance and Enhancement Team / Quality Enhancement  \n",
      "and Inclusion Service\n",
      "Version 1.0\n",
      "Document Status Approved\n",
      "Approved by Academic Regulations and \n",
      "Policy Committee\n",
      "Date 23 June 2022\n",
      "Date of Publication September 2022 Next Review Date July 2025\n",
      "Related \n",
      "Documents\n",
      "Extenuating Circumstances procedure\n",
      "Academic Regulations\n",
      "Assessment and Feedback Policy\n",
      "Amendments \n",
      "since approval\n",
      "Detail of revision Date Approved by\n",
      "5 Work that has been submitted on time, or during the late submission period, will \n",
      "be marked once the original deadline has passed and therefore a revised version, \n",
      "or additional elements, cannot be resubmitted after the deadline for a penalty.\n",
      "6 The School may approve the exclusion of some assessment components from\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If you submit your assignment 3 days late, the mark you receive will be reduced based on how late the submission is. Specifically, penalties apply for late submissions, but the exact reduction in marks is not detailed in the content provided. You may want to refer to the Late Submission of Assessment Policy for specific penalties.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What if I submit it 3 hours late?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_WrgpTsjTlqGuzBvSmiM6Mjak)\n",
      " Call ID: call_WrgpTsjTlqGuzBvSmiM6Mjak\n",
      "  Args:\n",
      "    query: penalties for submitting assignment 3 hours late\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'Data\\\\Policies\\\\LateAssessment.pdf', 'page': 1, 'page_label': '2', 'start_index': 798}\n",
      "Content: 2. P enalties for Late Submission\n",
      "1 Where a student submits a first attempt at an assessment between 1 and 24 \n",
      "hours after the published submission deadline, the original mark awarded will \n",
      "be reduced by 5%. For example, a mark of 60% will be reduced by 3 marks so \n",
      "that the mark the student will receive is 57%. This reduction will not be made \n",
      "if the original mark is below 40% [50% for postgraduate courses] or in cases \n",
      "where the deduction of 5% will reduce the mark from a pass to a fail. In such \n",
      "cases the mark awarded will be 40% [50% for postgraduate courses].\n",
      "2 Where a student submits a first attempt at an assessment between 24 hours \n",
      "and one week (5 working days) after the published submission deadline the \n",
      "original mark awarded will be reduced by 10%. For example, a mark of 60% \n",
      "will be reduced by 6 marks so that the mark the student will receive is 54%. \n",
      "This reduction will not be made if the original mark is below 40% [50% for\n",
      "\n",
      "Source: {'source': 'Data\\\\Policies\\\\LateAssessment.pdf', 'page': 2, 'page_label': '3', 'start_index': 789}\n",
      "Content: or additional elements, cannot be resubmitted after the deadline for a penalty.\n",
      "6 The School may approve the exclusion of some assessment components from \n",
      "the full late submissions scheme where the teaching pattern provides rapid \n",
      "feedback within 7 days of the original deadline. In such circumstances, late \n",
      "submission would only be permitted up to 24 hours of the original deadline, \n",
      "and not up to 7 days. Occasionally, where a feedback session is timetabled \n",
      "within 24 hours of the deadline, 24 hour late submission will not be possible. \n",
      "In addition, Schools may exclude other forms of assessment such as Time-\n",
      "Constrained Assessments (TCAs) where the short deadline set is an integral \n",
      "part of the learning outcome/s being assessed. In all such cases the Module \n",
      "Guide and/or Assessment Brief must make this clear to students at the start \n",
      "of the academic year.\n",
      "7 Submission of group submissions more than 1 hour after the published deadline\n",
      "\n",
      "Source: {'source': 'Data\\\\Policies\\\\LateAssessment.pdf', 'page': 1, 'page_label': '2', 'start_index': 0}\n",
      "Content: Late Submission of Assessment Policy\n",
      "2\n",
      " 2\n",
      "1. Intr oduction\n",
      "2\n",
      "1 Assessments must be submitted in the format specified in the assessment \n",
      "task, by the deadline and to the submission point published on Moodle. Where \n",
      "a student submits work after the published submission deadline, the mark they \n",
      "receive for the assessment will be reduced according to how long after the \n",
      "deadline they submit.\n",
      "2 1.2 A student may apply for an extension of time to complete assessed \n",
      "coursework if there are personal circumstances, which are unforeseen and \n",
      "unpreventable and have an effect on the student‚Äôs ability to submit work by \n",
      "the published hand-in deadline. Further detail on the process for making an \n",
      "Extenuating Circumstances claim for an extension is set out in the Extenuating \n",
      "Circumstance Procedure . \n",
      "2. P enalties for Late Submission\n",
      "1 Where a student submits a first attempt at an assessment between 1 and 24 \n",
      "hours after the published submission deadline, the original mark awarded will\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If you submit your assignment 3 hours late, your original mark will be reduced by 5%. For example, a mark of 60% would drop to 57%, unless the original mark is below 40% (or 50% for postgraduate courses) or if the reduction would lower it from a pass to a fail.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What if my mark was 78%\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If your original mark was 78% and you submitted your assignment 3 hours late, it would be reduced by 5%. \n",
      "\n",
      "So, the calculation would be:\n",
      "\n",
      "- Original mark: 78%\n",
      "- Reduction (5% of 78%): 3.9 marks\n",
      "- New mark: 78% - 3.9% = 74.1%\n",
      "\n",
      "You would receive a mark of approximately 74.1%.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What questions have I asked you in this conversation?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "In this conversation, you have asked the following questions:\n",
      "\n",
      "1. \"What happens if I submit my assignment 3 days late?\"\n",
      "2. \"What if I submit it 3 hours late?\"\n",
      "3. \"What if my mark was 78%?\" \n",
      "4. \"What questions have I asked you in this conversation?\" \n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Lewis!\n"
     ]
    }
   ],
   "source": [
    "state = graph.get_state(config).values\n",
    "\n",
    "for message in state[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
