{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Artificially Intelligent Chatbot\n",
    "## Week 3 Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current checklist:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checklist isn't exhaustive, more things may be added to it as time passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Data mining*\n",
    "  - üõ†Ô∏è Policies\n",
    "  - ‚ùå Campus locations\n",
    "  - ‚ùå Societies\n",
    "- *Data Storage*\n",
    "  - ‚úÖ Vector DB Identification\n",
    "  - ‚úÖ Embedding\n",
    "- *Chatbot*  \n",
    "  - ‚úÖ Data retrieval\n",
    "  - ‚úÖ Conversational memory\n",
    "  - ‚ùå User interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future plans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt engineering \n",
    "    - The prototype seems to work okay with its current prompt, but maybe it could be even better.\n",
    "    \n",
    "- More policy data\n",
    "    - A small selection of University policies can currently be queried, though not all of them.\n",
    "\n",
    "\n",
    "- Tools \n",
    "    - Currently, the LLM can use one tool (if it deems it necessary), which is to retrieve data from the DB.\n",
    "            Perhaps more tools can be added for general use, like telling the time? (How long until this deadline, etc)\n",
    "\n",
    "- Manual Data Creation\n",
    "    - While the LLM can gather some general information about the university from the policies it retrieves, it will be helpful (perhaps essential) to create an additional PDF of my own with some information about the university, such as the campus locations. This may also be the best way to encode data about societies? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and key variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported methods and classes are described in further detail when they're used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to get the OpenAI API key from the system environment variables.\n",
    "import os\n",
    "\n",
    "# Initialises the LLM.\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# The vector DB used for this prototype. It's worked well this far,\n",
    "# so I'll likely use it in future versions, too.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Allows for inputs to be sent to the LLM as a human (user input)\n",
    "# and the system (specialised prompt that defines LLM behaviour).\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Allows for \"tools\" to be created. The LLM can use these tools\n",
    "# to execute defined code, such as retrieving data from the FAISS DB.\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Used to embed queries to the FAISS DB.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Used to save the conversation to memory.\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangGraph key functionality. Declares a clear and consistent \n",
    "# structure for the chatbot. LangGraph is described in detail in \n",
    "# its own notebook section.\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Get the OpenAI API key from environment variables so that \n",
    "# it's not visible in this code on GitHub. OpenAI would revoke the key if it leaked.\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Sets the directory of the FAISS DB that's being loaded from.\n",
    "    # Options:\n",
    "    #   FAISS: Chunk size 1000, Overlap 200, UnstructuredPDFLoader in elements mode.\n",
    "    #   FAISS-PyPDF: Chunk size 1000, Overlap 200, PyPDFLoader with default args.\n",
    "FAISS_PATH = \"FAISS-PyPDF\"\n",
    "# Experimentation showed that using the PyPDFLoader-based DB gave better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS, embeddings and the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain allows for easy switching of embedding models by merely changing the model argument. However, using a different embedding model than the one used to create the vector database will have significant negative consequences that could render the chatbot inoperable, so it's essential that this matches what's used in the database embedding file, which is OpenAI's ``text-embedding-3-small`` in this prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the embedding model with the API key.\n",
    "embedder = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also allows for many different vector database options. This prototype uses a Facebook AI Similarity Search (FAISS) DB primarily due to its easy integration with LangChain - this single line of code is all that's necessary to retrieve the stored data from the chosen ``FAISS_PATH``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector database.\n",
    "db = FAISS.load_local(folder_path = FAISS_PATH,\n",
    "                      embeddings = embedder,\n",
    "                      allow_dangerous_deserialization=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS stores data in a Pickle file. This is a serialised format that allows Python to load the database. However, a malicious Pickle file can actually execute arbitrary code. The files used in this vector DB are not malicious, so it is fine to enable ``allow_dangerous_deserialization``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``init_chat_model``, as suggested by its name, initialises the model. This prototype, and most likely the final version, will use GPT-4o-mini due to its low cost in relation to other models. While it is a lower-quality model than higher-end models like GPT-4 or reasoning models like o1/o3-mini, it still can perform the simple chatbot functionalities of this prototype. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there's only one tool - the retriever itself, which will perform a semantic search on the FAISS DB based on the user's query. It returns the content of the 3 most similar chunks to the user's query, as well as which PDF they came from, though the user won't see that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format = \"content_and_artifact\")\n",
    "def retrieve(query):\n",
    "    # The docstring below is actually REQUIRED by LangGraph, and this \n",
    "    # won't run without it.\n",
    "    \"\"\"Retrieves the 3 most relevant chunks for the user's query.\"\"\"\n",
    "    retrieved_docs = db.similarity_search(query, k = 3)\n",
    "    \n",
    "    # The chunk's content and the document it came from (e.g \"Attendance.pdf\")\n",
    "    content = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return content, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph is a new option from the LangChain devs, which allow for the actions in the RAG chain to be directly plotted as a sequence of events.\n",
    "Primarily, it makes **conversational memory** extremely simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise an empty graph. Nodes and edges are added later.\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ``MessagesState`` is a list of messages, which contains the active conversation. They add a massive layer of abstraction in programming the chatbot, as no management of the conversation history is performed in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_or_respond(state: MessagesState):\n",
    "    # Allows the LLM to use the retrieve tool that was created earlier.\n",
    "    rag_llm = llm.bind_tools([retrieve])\n",
    "    \n",
    "    # The LLM decides on its own if it needs\n",
    "    response = rag_llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    # A key element of using a MessagesState is that that will append\n",
    "    # the response to the conversation history rather than overwriting.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# A node is created for the retrieval tool.\n",
    "# It's called tools despite only being one tool in case I make more later.\n",
    "# If I made more they'd be part of this list.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    # To massively reduce token consumption (and therefore cost),\n",
    "    # the most recent RAG context from tool calls is added to the \n",
    "    # prompt to stop the LLM searching the entire conversation history \n",
    "    # for something that was JUST said. \n",
    "    recent_tool_messages = []\n",
    "    \n",
    "    # To get the most recent ones, the list needs to be reversed\n",
    "    # so that the most recent come first instead of last.\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            # If it's a normal message, stop.\n",
    "            break\n",
    "    \n",
    "    # Put the tool messages in their original order in case \n",
    "    # the sequence of the retrieved context mattered. \n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Saves the context from the recent tool messages.\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    \n",
    "    # This is the LLM's system prompt, which decides how the LLM behaves.\n",
    "    system_message_content = (\n",
    "        # This is formatted like this to follow the Ruff linter's line length rule.\n",
    "        \"You are an assistant to help new students get acclimated to Birmingham City \"\n",
    "        \"University. Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. When referring to context, be specific and quote the context. \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\" # RAG context is attached here\n",
    "    )\n",
    "    \n",
    "    # The list of messages in the conversation.\n",
    "    # Only adds messages that AREN'T tool calls, as tool calls\n",
    "    # would hugely increase the input tokens used, and the LLM \n",
    "    # should (hopefully) have already said the useful info in its response\n",
    "    # so it can use that instead.\n",
    "    conversation_messages = [\n",
    "        message for message in state[\"messages\"] \n",
    "        \n",
    "        if message.type in (\"human\", \"system\") \n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    \n",
    "    # The final prompt consists of the system prompt and the conversation.\n",
    "    # This does mean that as a conversation continues, token cost will greatly increase.\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Get the LLM's response to the prompt and return the response.\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After establishing all the functions that form the chatbot, the graph can be built using the graph_builder `StateGraph` that was made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e5237a66b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add all the nodes.\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``query_or_respond`` has **conditions**:\n",
    "- If the user's query needs RAG context, the retrieve tool will be called.\n",
    "- If it does not, the LLM will generate a response by itself.\n",
    "\n",
    "\"What is the late submission deadline?\" invokes the retrieval tool. \n",
    "\n",
    "\"Hello!\" does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1e5237a66b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition, \n",
    "    # If retrieval is not needed, generate a general answer.\n",
    "    # If it is, call the retrieval tool.\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "\n",
    "# An edge is also needed between the tool call and response generation\n",
    "# to ensure the response has the RAG context.\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "\n",
    "# After the response is generated, the graph is done.\n",
    "graph_builder.add_edge(\"generate\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about LangGraph and memory, I used their documentation at https://langchain-ai.github.io/langgraph/concepts/persistence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to keep the ongoing conversation in memory.\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Thread ID would allow for multiple sessions of the chatbot to run simultaneously.\n",
    "# Different thread IDs have their own memory.\n",
    "config = {\"configurable\": {\"thread_id\": \"W3Prototyping\"}}\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile(checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query):\n",
    "    input_messages = [HumanMessage(query)]\n",
    "    output = graph.invoke({\"messages\": input_messages}, config)\n",
    "    output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype Main Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running all earlier cells, the cell below contains an infinite loop (broken by inputting \"quit\" or CTRL+C) to prompt the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    q = input(\"What is your query?\")\n",
    "    if q != \"quit\":\n",
    "        query(q)\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below outputs the entire conversation. It does include tool messages and the context they retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config).values\n",
    "\n",
    "for message in state[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
