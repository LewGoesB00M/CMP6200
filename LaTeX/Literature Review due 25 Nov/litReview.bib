@article{litSearch,
	title = {Where you search determines what you find: the effects of bibliographic databases on systematic reviews},
	volume = {25},
	issn = {1364-5579},
	url = {https://doi.org/10.1080/13645579.2021.1892378},
	doi = {10.1080/13645579.2021.1892378},
	shorttitle = {Where you search determines what you find},
	abstract = {Systematic literature reviews are common in social research for integrating and synthesising existing research. This paper argues that the outcomes of such reviews are affected by the choice of bibliographic databases. It presents evidence of substantial variation across three large electronic databases (Scopus, Web of Science and {EBSCO}) in a study on employee retention and staff turnover. It considers the specific articles, numbers returned, numbers shared across databases and perceived quality of journals hosting the retrieved articles. Results show that only 130 articles (5.7\% of 2267 retrieved) were found common to all three databases, suggesting that decisions on how and where literature is retrieved can substantially affect the results of systematic reviews and meta-analyses. The findings caution against the use of single databases and claiming comprehensiveness. The paper reflects on how additional literature search methods (e.g., contacting experts, citation indices) and their sequence of use can affect systematic review quality.},
	pages = {409--422},
	number = {3},
	journaltitle = {International Journal of Social Research Methodology},
	author = {Wanyama, Seperia B. and McQuaid, Ronald W. and Kittler, Markus},
	urldate = {2024-10-31},
	date = {2022-05-04},
	publisher = Routledge,
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\W7Z7IEHF\\Wanyama et al. - 2022 - Where you search determines what you find the effects of bibliographic databases on systematic revi.pdf:application/pdf},
}


@article{AIEthics,
	title = {Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered {AI} Systems},
	volume = {10},
	issn = {2160-6455},
	url = {https://dl.acm.org/doi/10.1145/3419764},
	doi = {10.1145/3419764},
	shorttitle = {Bridging the Gap Between Ethics and Practice},
	abstract = {This article attempts to bridge the gap between widely discussed ethical principles of Human-centered {AI} ({HCAI}) and practical steps for effective governance. Since {HCAI} systems are developed and implemented in multiple organizational structures, I propose 15 recommendations at three levels of governance: team, organization, and industry. The recommendations are intended to increase the reliability, safety, and trustworthiness of {HCAI} systems: (1) reliable systems based on sound software engineering practices, (2) safety culture through business management strategies, and (3) trustworthy certification by independent oversight. Software engineering practices within teams include audit trails to enable analysis of failures, software engineering workflows, verification and validation testing, bias testing to enhance fairness, and explainable user interfaces. The safety culture within organizations comes from management strategies that include leadership commitment to safety, hiring and training oriented to safety, extensive reporting of failures and near misses, internal review boards for problems and future plans, and alignment with industry standard practices. The trustworthiness certification comes from industry-wide efforts that include government interventions and regulation, accounting firms conducting external audits, insurance companies compensating for failures, non-governmental and civil society organizations advancing design principles, and professional organizations and research institutes developing standards, policies, and novel ideas. The larger goal of effective governance is to limit the dangers and increase the benefits of {HCAI} to individuals, organizations, and society.},
	pages = {26:1--26:31},
	number = {4},
	journaltitle = {{ACM} Trans. Interact. Intell. Syst.},
	author = {Shneiderman, Ben},
	urldate = {2024-10-31},
	date = {2020-10-16},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\HDDRHZGV\\Shneiderman - 2020 - Bridging the Gap Between Ethics and Practice Guidelines for Reliable, Safe, and Trustworthy Human-c.pdf:application/pdf},
}

@article{AIDigitalAssistants,
	title = {{AI}-Based Digital Assistants},
	volume = {61},
	issn = {1867-0202},
	url = {https://doi.org/10.1007/s12599-019-00600-8},
	doi = {10.1007/s12599-019-00600-8},
	pages = {535--544},
	number = {4},
	journaltitle = {Business \& Information Systems Engineering},
	shortjournal = {Bus Inf Syst Eng},
	author = {Maedche, Alexander and Legner, Christine and Benlian, Alexander and Berger, Benedikt and Gimpel, Henner and Hess, Thomas and Hinz, Oliver and Morana, Stefan and Söllner, Matthias},
	urldate = {2024-10-31},
	date = {2019-08-01},
	langid = {english},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\U6RZZZAC\\Maedche et al. - 2019 - AI-Based Digital Assistants.pdf:application/pdf},
}

% You may notice that some of these entries are formatted differently. This is because I switched to using Zotero which 
% generates them for me with different naming schemes.

@inproceedings{vaswani_attention_2017,
	location = {Red Hook, {NY}, {USA}},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	series = {{NIPS}'17},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	pages = {6000--6010},
	doi = {10.48550/arXiv.1706.03762},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2024-11-04},
	date = {2017-12-04},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\JR84UK2N\\Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf},
	keywords = {refs}
}


@article{turing_icomputing_1950,
	title = {I.—{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
	volume = {{LIX}},
	issn = {1460-2113, 0026-4423},
	url = {https://academic.oup.com/mind/article/LIX/236/433/986238},
	doi = {10.1093/mind/LIX.236.433},
	pages = {433--460},
	number = {236},
	journaltitle = {Mind},
	author = {Turing, A. M.},
	urldate = {2024-11-04},
	date = {1950-10-01},
	langid = {english},
	keywords = {refs},
	file = {PDF:C\:\\Users\\Lewis\\Zotero\\storage\\EQELPP9J\\Turing - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf:application/pdf},
}


@inproceedings{selbst_fairness_2019,
	location = {New York, {NY}, {USA}},
	title = {Fairness and Abstraction in Sociotechnical Systems},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287598},
	doi = {10.1145/3287560.3287598},
	series = {{FAT}* '19},
	abstract = {A key goal of the fair-{ML} community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-{ML} work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
	pages = {59--68},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
	urldate = {2024-11-06},
	date = {2019-01-29},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\EFZC4IQ7\\Selbst et al. - 2019 - Fairness and Abstraction in Sociotechnical Systems.pdf:application/pdf},
}


@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2024-11-06},
	date = {2012},
	doi = {10.1145/3065386},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\EMSCN29N\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf},
}


@article{samuel_studies_1959,
	title = {Some Studies in Machine Learning Using the Game of Checkers},
	volume = {3},
	issn = {0018-8646},
	url = {https://ieeexplore.ieee.org/abstract/document/5392560},
	doi = {10.1147/rd.33.0210},
	abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
	pages = {210--229},
	number = {3},
	journaltitle = {{IBM} Journal of Research and Development},
	author = {Samuel, A. L.},
	urldate = {2024-11-06},
	date = {1959-07},
	note = {Conference Name: {IBM} Journal of Research and Development},
	keywords = {refs},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Lewis\\Zotero\\storage\\RTFHX2SY\\5392560.html:text/html;Submitted Version:C\:\\Users\\Lewis\\Zotero\\storage\\TLDDTG9K\\Samuel - 1959 - Some Studies in Machine Learning Using the Game of Checkers.pdf:application/pdf},
}


@article{wirtz_brave_2018,
	title = {Brave new world: service robots in the frontline},
	volume = {29},
	doi = {10.1108/JOSM-04-2018-0119},
	shorttitle = {Brave new world},
	abstract = {Purpose: The service sector is at an inflection point with regard to productivity gains and service industrialization similar to the industrial revolution in manufacturing that started in the eighteenth century. Robotics in combination with rapidly improving technologies like artificial intelligence ({AI}), mobile, cloud, big data and biometrics will bring opportunities for a wide range of innovations that have the potential to dramatically change service industries. The purpose of this paper is to explore the potential role service robots will play in the future and to advance a research agenda for service researchers. Design/methodology/approach: This paper uses a conceptual approach that is rooted in the service, robotics and {AI} literature. Findings: The contribution of this paper is threefold. First, it provides a definition of service robots, describes their key attributes, contrasts their features and capabilities with those of frontline employees, and provides an understanding for which types of service tasks robots will dominate and where humans will dominate. Second, this paper examines consumer perceptions, beliefs and behaviors as related to service robots, and advances the service robot acceptance model. Third, it provides an overview of the ethical questions surrounding robot-delivered services at the individual, market and societal level. Practical implications: This paper helps service organizations and their management, service robot innovators, programmers and developers, and policymakers better understand the implications of a ubiquitous deployment of service robots. Originality/value: This is the first conceptual paper that systematically examines key dimensions of robot-delivered frontline service and explores how these will differ in the future. © 2018, Jochen Wirtz, Paul G. Patterson, Werner H. Kunz, Thorsten Gruber, Vinh Nhat Lu, Stefanie Paluch and Antje Martins.},
	pages = {907--931},
	number = {5},
	journaltitle = {Journal of Service Management},
	author = {Wirtz, J. and Patterson, P.G. and Kunz, W.H. and Gruber, T. and Lu, V.N. and Paluch, S. and Martins, A.},
	date = {2018},
	keywords = {Artificial intelligence, Consumer behaviour, Ethics, Markets, Privacy, Service robots, refs},
	file = {Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\4QYKKXGC\\Wirtz et al. - 2018 - Brave new world service robots in the frontline.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\FCSMRGYF\\display.html:text/html},
}


@article{miro-nicolau_comprehensive_2025,
	title = {A comprehensive study on fidelity metrics for {XAI}},
	volume = {62},
	doi = {10.1016/j.ipm.2024.103900},
	abstract = {The use of {eXplainable} Artificial Intelligence ({XAI}) systems has introduced a set of challenges that need resolution. Herein, we focus on how to correctly select an {XAI} method, an open questions within the field. The inherent difficulty of this task is due to the lack of a ground truth. Several authors have proposed metrics to approximate the fidelity of different {XAI} methods. These metrics lack verification and have concerning disagreements. In this study, we proposed a novel methodology to verify fidelity metrics, using transparent models. These models allowed us to obtain explanations with perfect fidelity. Our proposal constitutes the first objective benchmark for these metrics, facilitating a comparison of existing proposals, and surpassing existing methods. We applied our benchmark to assess the existing fidelity metrics in two different experiments, each using public datasets comprising 52,000 images. The images from these datasets had a size a 128 by 128 pixels and were synthetic data that simplified the training process. We identified that two fidelity metrics, Faithfulness Estimate and Faithfulness Correlation, obtained the expected perfect results for linear models, showing their ability to approximate fidelity for this kind of methods. However, when present with non-linear models, as the ones most used in the state-of-the-art,all metric values, indicated a lack of fidelity, with the best one showing a 30\% deviation from the expected values for perfect explanation. Our experimentation led us to conclude that the current fidelity metrics are not reliable enough to be used in real scenarios. From this finding, we deemed it necessary to development new metrics, to avoid the detected problems, and we recommend the usage of our proposal as a benchmark within the scientific community to address these limitations. © 2024 The Authors},
	number = {1},
	journaltitle = {Information Processing and Management},
	author = {Miró-Nicolau, M. and Jaume-i-Capó, A. and Moyà-Alcover, G.},
	date = {2025},
	keywords = {Explainable Artificial Intelligence ({XAI}), Fidelity, Objective evaluation, refs},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\TUEXV8IB\\Miró-Nicolau et al. - 2025 - A comprehensive study on fidelity metrics for XAI.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\5CWT8N4X\\display.html:text/html},
}


@inproceedings{wang_fine-grained_2016,
	title = {Fine-grained sentiment analysis of social media with emotion sensing},
	url = {https://ieeexplore.ieee.org/document/7821783},
	doi = {10.1109/FTC.2016.7821783},
	abstract = {Social media is arguably the richest source of human generated text input. Opinions, feedbacks and critiques provided by internet users reflect attitudes and sentiments towards certain topics, products, or services. The sheer volume of such information makes it effectively impossible for any group of persons to read through. Thus, social media sentiment analysis has become an important area of work to make sense of the social media talk. However, most existing sentiment analysis techniques focus only on the aggregate level, classifying sentiments broadly into positive, neutral or negative, and lack the capabilities to perform fine-grained sentiment analysis. This paper describes a social media analytics engine that employs a social adaptive fuzzy similarity-based classification method to automatically classify text messages into sentiment categories (positive, negative, neutral and mixed), with the ability to identify their prevailing emotion categories (e.g., satisfaction, happiness, excitement, anger, sadness, and anxiety). It is also embedded within an end-to-end social media analysis system that has the capabilities to collect, filter, classify, and analyze social media text data and display a descriptive and predictive analytics dashboard for a given concept. The proposed method has been developed and is ready to be licensed to users.},
	eventtitle = {2016 Future Technologies Conference ({FTC})},
	pages = {1361--1364},
	booktitle = {2016 Future Technologies Conference ({FTC})},
	author = {Wang, Zhaoxia and Chong, Chee Seng and Lan, Landy and Yang, Yinping and Beng Ho, Seng and Tong, Joo Chuan},
	urldate = {2024-11-06},
	date = {2016-12},
	keywords = {emotion, Engines, Information filters, Learning systems, opinion mining, Sensors, sentiment analysis, Sentiment analysis, sentiment classification, social adaptive fuzzy similarity, social media, Social network services, refs},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Lewis\\Zotero\\storage\\8XQIMKTP\\7821783.html:text/html},
}


@article{collobert_natural_2011,
	title = {Natural Language Processing (Almost) from Scratch},
	volume = {12},
	issn = {1532-4435},
	url = {https://dl-acm-org.bcu.idm.oclc.org/doi/10.5555/1953048.2078186},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	pages = {2493--2537},
	issue = {2/1/2011},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Léon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	date = {2011-11-01},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\UL2VSE7A\\Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:application/pdf},
}


@incollection{zampolli_natural_1994,
	location = {Dordrecht},
	title = {Natural Language Processing: A Historical Review},
	isbn = {978-0-7923-2998-5 978-0-585-35958-8},
	url = {http://link.springer.com/10.1007/978-0-585-35958-8_1},
	shorttitle = {Natural Language Processing},
	abstract = {This paper reviews natural language processing ({NLP}) from the late 1940's to the present, seeking to identify its successive trends as these reflect concerns with different problems or the pursuit of different approaches to solving these problems and building systems as wholes. The review distinguishes four phases in the history of {NLP}, characterised respectively by an emphasis on machine translation, by the influence of artificial intelligence, by the adoption of a logico-grammatical style, and by an attack on massive language data. The account considers the significant and salient work in each phase, and concludes with an assessment of where we stand after more than forty years of effort in the field.},
	pages = {3--16},
	booktitle = {Current Issues in Computational Linguistics: In Honour of Don Walker},
	publisher = {Springer Netherlands},
	author = {Jones, Karen Sparck},
	editor = {Zampolli, Antonio and Calzolari, Nicoletta and Palmer, Martha},
	urldate = {2024-11-06},
	date = {1994},
	langid = {english},
	doi = {10.1007/978-0-585-35958-8_1},
	file = {PDF:C\:\\Users\\Lewis\\Zotero\\storage\\A4TEFKSR\\Jones - 1994 - Natural Language Processing A Historical Review.pdf:application/pdf},
}


@inproceedings{abadi_tensorflow_2016,
	location = {Savannah, {GA}},
	title = {{TensorFlow}: A System for Large-Scale Machine Learning},
	isbn = {978-1-931971-33-1},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	doi = {10.48550/arXiv.1605.08695},
	pages = {265--283},
	booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
	publisher = {{USENIX} Association},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	date = {2016-11},
	file = {PDF:C\:\\Users\\Lewis\\Zotero\\storage\\4FWHMPTG\\Abadi et al. - TensorFlow A System for Large-Scale Machine Learning.pdf:application/pdf},
}


@inproceedings{tang_document_2015,
	location = {Lisbon, Portugal},
	title = {Document Modeling with Gated Recurrent Neural Network for Sentiment Classification},
	url = {https://aclanthology.org/D15-1167},
	doi = {10.18653/v1/D15-1167},
	eventtitle = {{EMNLP} 2015},
	pages = {1422--1432},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
	urldate = {2024-11-06},
	date = {2015-09},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\X3HKUTER\\Tang et al. - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:application/pdf},
}

@misc{dubey_llama_2024,
	title = {The Llama 3 Herd of Models},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence ({AI}) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as {GPT}-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	number = {{arXiv}:2407.21783},
	publisher = {{arXiv}},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and {McConnell}, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and {AlBadawy}, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzmán, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and {McPhie}, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and {DeVito}, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
	urldate = {2024-11-07},
	date = {2024-08-15},
	eprinttype = {arxiv},
	eprint = {2407.21783},
	keywords = {refs, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Lewis\\Zotero\\storage\\ZG29H9VA\\Dubey et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\G9NBSJET\\2407.html:text/html},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	number = {{arXiv}:2203.02155},
	publisher = {{arXiv}},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	urldate = {2024-11-10},
	date = {2022-03-04},
	eprinttype = {arxiv},
	eprint = {2203.02155},
	keywords = {refs, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Lewis\\Zotero\\storage\\75NKNNSK\\Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\D4RMKF3E\\2203.html:text/html},
}


@article{dwivedi_so_2023,
	title = {“So what if {ChatGPT} wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational {AI} for research, practice and policy},
	volume = {71},
	doi = {10.1016/j.ijinfomgt.2023.102642},
	shorttitle = {“So what if {ChatGPT} wrote it?},
	abstract = {Transformative artificially intelligent tools, such as {ChatGPT}, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge {ChatGPT}'s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether {ChatGPT}'s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative {AI}; examining biases of generative {AI} attributable to training datasets and processes; exploring business and societal contexts best suited for generative {AI} implementation; determining optimal combinations of human and generative {AI} for various tasks; identifying ways to assess accuracy of text produced by generative {AI}; and uncovering the ethical and legal issues in using generative {AI} across different contexts. © 2023 The Authors},
	journaltitle = {International Journal of Information Management},
	author = {Dwivedi, Y.K. and Kshetri, N. and Hughes, L. and Slade, E.L. and Jeyaraj, A. and Kar, A.K. and Baabdullah, A.M. and Koohang, A. and Raghavan, V. and Ahuja, M. and Albanna, H. and Albashrawi, M.A. and Al-Busaidi, A.S. and Balakrishnan, J. and Barlette, Y. and Basu, S. and Bose, I. and Brooks, L. and Buhalis, D. and Carter, L. and Chowdhury, S. and Crick, T. and Cunningham, S.W. and Davies, G.H. and Davison, R.M. and Dé, R. and Dennehy, D. and Duan, Y. and Dubey, R. and Dwivedi, R. and Edwards, J.S. and Flavián, C. and Gauld, R. and Grover, V. and Hu, M.-C. and Janssen, M. and Jones, P. and Junglas, I. and Khorana, S. and Kraus, S. and Larsen, K.R. and Latreille, P. and Laumer, S. and Malik, F.T. and Mardani, A. and Mariani, M. and Mithas, S. and Mogaji, E. and Nord, J.H. and O'Connor, S. and Okumus, F. and Pagani, M. and Pandey, N. and Papagiannidis, S. and Pappas, I.O. and Pathak, N. and Pries-Heje, J. and Raman, R. and Rana, N.P. and Rehm, S.-V. and Ribeiro-Navarrete, S. and Richter, A. and Rowe, F. and Sarker, S. and Stahl, B.C. and Tiwari, M.K. and van der Aalst, W. and Venkatesh, V. and Viglia, G. and Wade, M. and Walton, P. and Wirtz, J. and Wright, R.},
	date = {2023},
	keywords = {refs, {ChatGPT}, Conversational agent, Generative {AI}, Generative artificial intelligence, Large language models},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\EBECUKWR\\Dwivedi et al. - 2023 - “So what if ChatGPT wrote it” Multidisciplinary perspectives on opportunities, challenges and impli.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\6Z6XGMDZ\\display.html:text/html},
}


@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	number = {{arXiv}:2005.11401},
	publisher = {{arXiv}},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2024-11-10},
	date = {2021-04-12},
	eprinttype = {arxiv},
	eprint = {2005.11401},
	keywords = {refs, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Lewis\\Zotero\\storage\\D6J65Q3M\\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\P6USX7KJ\\2005.html:text/html},
}

@inproceedings{laugwitz_construction_2008,
	location = {Berlin, Heidelberg},
	title = {Construction and Evaluation of a User Experience Questionnaire},
	isbn = {978-3-540-89350-9},
	doi = {10.1007/978-3-540-89350-9_6},
	abstract = {An end-user questionnaire to measure user experience quickly in a simple and immediate way while covering a preferably comprehensive impression of the product user experience was the goal of the reported construction process. An empirical approach for the item selection was used to ensure practical relevance of items. Usability experts collected terms and statements on user experience and usability, including ‘hard’ as well as ‘soft’ aspects. These statements were consolidated and transformed into a first questionnaire version containing 80 bipolar items. It was used to measure the user experience of software products in several empirical studies. Data were subjected to a factor analysis which resulted in the construction of a 26 item questionnaire including the six factors Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and Novelty. Studies conducted for the original German questionnaire and an English version indicate a satisfactory level of reliability and construct validity.},
	pages = {63--76},
	booktitle = {{HCI} and Usability for Education and Work},
	publisher = {Springer},
	author = {Laugwitz, Bettina and Held, Theo and Schrepp, Martin},
	editor = {Holzinger, Andreas},
	date = {2008},
	langid = {english},
	keywords = {refs, Perceived usability, Questionnaire, Software evaluation, Usability assessment, User experience, User satisfaction},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\P767UFQN\\Laugwitz et al. - 2008 - Construction and Evaluation of a User Experience Questionnaire.pdf:application/pdf},
}


@inproceedings{kotian_systematic_2024,
	title = {A Systematic Review on Human and Computer Interaction},
	url = {https://ieeexplore.ieee.org/document/10467622},
	doi = {10.1109/IDCIoT59759.2024.10467622},
	abstract = {As technology continues to advance at an unprecedented pace, the interaction between humans and computers has become an integral part of our daily lives. This study provides a comprehensive review of the evolving landscape of human-computer interaction ({HCI}) research, focusing on the key concepts, methodologies, and advancements in this interdisciplinary field. The review begins by presenting an overview of the historical evolution of {HCI}, tracing its roots from early command-line interfaces to the current era of intuitive touchscreens and voice recognition systems. The fundamental principles of {HCI}, including usability, accessibility, and user-centered design, are examined in detail, highlighting their significance in enhancing the overall user experience. Moreover, the review explores various interaction modalities that have emerged over the years, such as graphical user interfaces, haptic feedback, augmented reality, and virtual reality. It examines the strengths, limitations, and potential applications of these modalities, shedding light on the future possibilities they hold for human-computer interaction. Furthermore, the review delves into the emerging trends in {HCI} research, including natural language processing, gesture recognition, machine learning, and affective computing. These advancements have paved the way for more personalized and adaptive interfaces, enabling computers to understand and respond to human emotions and intentions, thereby fostering deeper levels of engagement and satisfaction. The study also addresses the challenges and ethical considerations associated with human-computer interaction, such as privacy concerns, data security, and algorithmic biases. It emphasizes the importance of designing inclusive and ethical systems that respect users' rights and values.},
	eventtitle = {2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things ({IDCIoT})},
	pages = {1214--1218},
	booktitle = {2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things ({IDCIoT})},
	author = {Kotian, Abhijith L and Nandipi, Reshna and M, Ushag and S, Usha Rani and {VARSHAUK} and T, Veena G},
	urldate = {2024-11-11},
	date = {2024-01},
	keywords = {refs, Augmented Reality ({AR}), Computers, Ethics, Graphical User Interfaces ({GUI}), Human-computer interaction ({HCI}), Reviews, Speech recognition, Systematics, Touch sensitive screens, User centered design, User-centered Design ({UCD}), Virtual Reality ({VR})},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\JALLCN3B\\Kotian et al. - 2024 - A Systematic Review on Human and Computer Interaction.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Lewis\\Zotero\\storage\\Z5HX6REZ\\10467622.html:text/html},
}


@inproceedings{putnam_how_2012,
	location = {New York, {NY}, {USA}},
	title = {How do professionals who create computing technologies consider accessibility?},
	isbn = {978-1-4503-1321-6},
	url = {https://dl.acm.org/doi/10.1145/2384916.2384932},
	doi = {10.1145/2384916.2384932},
	series = {{ASSETS} '12},
	abstract = {In this paper, we present survey findings about how user experience ({UX}) and human-computer interaction ({HCI}) professionals, who create information and communication technologies ({ICTs}), reported considering accessibility in their work. Participants (N = 199) represented a wide range of job titles and nationalities. We found that most respondents (87\%, N = 173) reported that accessibility was important or very important in their work; however, when considerations for accessibility were discussed in an open-ended question (N =185) the scope was limited. Additionally, we found that aspects of empathy and professional experience were associated with how accessibility considerations were reported. We also found that many respondents indicated that decisions about accessibility were not in their control. We argue that a better understanding about how accessibility is considered by professionals has implications for academic programs in {HCI} and {UX} as to how well programs are preparing students to consider and advocate for inclusive design.},
	pages = {87--94},
	booktitle = {Proceedings of the 14th international {ACM} {SIGACCESS} conference on Computers and accessibility},
	publisher = {Association for Computing Machinery},
	author = {Putnam, Cynthia and Wozniak, Kathryn and Zefeldt, Mary Jo and Cheng, Jinghui and Caputo, Morgan and Duffield, Carl},
	urldate = {2024-11-11},
	date = {2012-10-22},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\56KGJF2W\\Putnam et al. - 2012 - How do professionals who create computing technologies consider accessibility.pdf:application/pdf},
}


@inproceedings{zamfirescu-pereira_why_2023,
	location = {New York, {NY}, {USA}},
	title = {Why Johnny Can’t Prompt: How Non-{AI} Experts Try (and Fail) to Design {LLM} Prompts},
	isbn = {978-1-4503-9421-5},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581388},
	doi = {10.1145/3544548.3581388},
	series = {{CHI} '23},
	shorttitle = {Why Johnny Can’t Prompt},
	abstract = {Pre-trained large language models (“{LLMs}”) like {GPT}-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer {LLM} outputs (“prompting”) has emerged as an important design technique potentially accessible to non-{AI}-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-{AI}-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype {LLM}-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-{AI}-expert-facing {LLM}-based tool design and for improving {LLM}-and-prompt literacy among programmers and the public, and present opportunities for further research.},
	pages = {1--21},
	booktitle = {Proceedings of the 2023 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
	urldate = {2024-11-11},
	date = {2023-04-19},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\WTDQFDZB\\Zamfirescu-Pereira et al. - 2023 - Why Johnny Can’t Prompt How Non-AI Experts Try (and Fail) to Design LLM Prompts.pdf:application/pdf},
}


@inproceedings{luger_like_2016,
	location = {New York, {NY}, {USA}},
	title = {"Like Having a Really Bad {PA}": The Gulf between User Expectation and Experience of Conversational Agents},
	isbn = {978-1-4503-3362-7},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858288},
	doi = {10.1145/2858036.2858288},
	series = {{CHI} '16},
	shorttitle = {"Like Having a Really Bad {PA}"},
	abstract = {The past four years have seen the rise of conversational agents ({CAs}) in everyday life. Apple, Microsoft, Amazon, Google and Facebook have all embedded proprietary {CAs} within their software and, increasingly, conversation is becoming a key mode of human-computer interaction. Whilst we have long been familiar with the notion of computers that speak, the investigative concern within {HCI} has been upon multimodality rather than dialogue alone, and there is no sense of how such interfaces are used in everyday life. This paper reports the findings of interviews with 14 users of {CAs} in an effort to understand the current interactional factors affecting everyday use. We find user expectations dramatically out of step with the operation of the systems, particularly in terms of known machine intelligence, system capability and goals. Using Norman's 'gulfs of execution and evaluation' [30] we consider the implications of these findings for the design of future systems.},
	pages = {5286--5297},
	booktitle = {Proceedings of the 2016 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Luger, Ewa and Sellen, Abigail},
	urldate = {2024-11-10},
	date = {2016-05-07},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\G44KWCG4\\Luger and Sellen - 2016 - Like Having a Really Bad PA The Gulf between User Expectation and Experience of Conversational Ag.pdf:application/pdf},
}


@article{chammas_closer_2015,
	title = {A Closer Look on the User Centred Design},
	volume = {3},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978915006575},
	doi = {10.1016/j.promfg.2015.07.656},
	series = {6th International Conference on Applied Human Factors and Ergonomics ({AHFE} 2015) and the Affiliated Conferences, {AHFE} 2015},
	abstract = {This paper discusses the User-Centred Design approach. It presents {UCD} assumptions and concepts, the benefits for users of the products developed under it, and the complications that this practice can bring, since implies in constant iterations and user omnipresence. As a result, we discuss a possible increment of time and budget as well as reviews on this approach front to the market urgency of technology and innovation. It can be seen that although each situation deserves appropriate adjustments for the profile, the best design is still the user-centred. Examples of this approach application with children enhance the discussion over its flexibility and the gains of design projects oriented by user needs and desires.},
	pages = {5397--5404},
	journaltitle = {Procedia Manufacturing},
	shortjournal = {Procedia Manufacturing},
	author = {Chammas, Adriana and Quaresma, Manuela and Mont’Alvão, Cláudia},
	urldate = {2024-11-11},
	date = {2015-01-01},
	keywords = {refs, Children, Methodology, User-Centred Design},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\KAWKMMTB\\Chammas et al. - 2015 - A Closer Look on the User Centred Design.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\8FXHL75Q\\S2351978915006575.html:text/html},
}


@inproceedings{clark_what_2019,
	location = {New York, {NY}, {USA}},
	title = {What Makes a Good Conversation? Challenges in Designing Truly Conversational Agents},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300705},
	doi = {10.1145/3290605.3300705},
	series = {{CHI} '19},
	shorttitle = {What Makes a Good Conversation?},
	abstract = {Conversational agents promise conversational interaction but fail to deliver. Efforts often emulate functional rules from human speech, without considering key characteristics that conversation must encapsulate. Given its potential in supporting long-term human-agent relationships, it is paramount that {HCI} focuses efforts on delivering this promise. We aim to understand what people value in conversation and how this should manifest in agents. Findings from a series of semi-structured interviews show people make a clear dichotomy between social and functional roles of conversation, emphasising the long-term dynamics of bond and trust along with the importance of context and relationship stage in the types of conversations they have. People fundamentally questioned the need for bond and common ground in agent communication, shifting to more utilitarian definitions of conversational qualities. Drawing on these findings we discuss key challenges for conversational agent design, most notably the need to redefine the design parameters for conversational agent interaction.},
	pages = {1--12},
	booktitle = {Proceedings of the 2019 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Clark, Leigh and Pantidi, Nadia and Cooney, Orla and Doyle, Philip and Garaialde, Diego and Edwards, Justin and Spillane, Brendan and Gilmartin, Emer and Murad, Christine and Munteanu, Cosmin and Wade, Vincent and Cowan, Benjamin R.},
	urldate = {2024-11-11},
	date = {2019-05-02},
	keywords = {refs},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\EXWP9RYD\\Clark et al. - 2019 - What Makes a Good Conversation Challenges in Designing Truly Conversational Agents.pdf:application/pdf},
}


@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2024-11-11},
	keywords = {refs},
	note = {I can't actually get access to this one},
	date = {1997-11-01},
}


@article{sherstinsky_fundamentals_2020,
	title = {Fundamentals of Recurrent Neural Network ({RNN}) and Long Short-Term Memory ({LSTM}) network},
	volume = {404},
	issn = {0167-2789},
	url = {https://www.sciencedirect.com/science/article/pii/S0167278919305974},
	doi = {10.1016/j.physd.2019.132306},
	abstract = {Because of their effectiveness in broad practical applications, {LSTM} networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the {LSTM} network and its parent, {RNN}, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an {RNN} is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential {RNN} and {LSTM} fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical {RNN} formulation from differential equations. We then propose and prove a precise statement, which yields the {RNN} unrolling technique. We also review the difficulties with training the standard {RNN} and address them by transforming the {RNN} into the “Vanilla {LSTM}”1 1The nickname “Vanilla {LSTM}” symbolizes this model’s flexibility and generality (Greff et al., 2015). network through a series of logical arguments. We provide all equations pertaining to the {LSTM} system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the {LSTM} system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the {LSTM} system and incorporate these extensions into the Vanilla {LSTM} network, producing the most general {LSTM} variant to date. The target reader has already been exposed to {RNNs} and {LSTM} networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented {LSTM} model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
	pages = {132306},
	journaltitle = {Physica D: Nonlinear Phenomena},
	shortjournal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	urldate = {2024-11-11},
	date = {2020-03-01},
	keywords = {refs, Convolutional input context windows, External input gate, {LSTM}, {RNN}, {RNN} unfolding/unrolling},
	file = {ScienceDirect Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\DYDMNWWK\\S0167278919305974.html:text/html;Submitted Version:C\:\\Users\\Lewis\\Zotero\\storage\\PJUIPYIA\\Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network.pdf:application/pdf},
}



@online{IBMAIDef,
    author = {IBM},
    title = {What is AI?},
    date = {2024-08-16},
    url = {https://www.ibm.com/topics/artificial-intelligence},
    urldate = {2024-10-28},
    keywords = {bib}
}

@online{ICOAIDef,
    author = {ICO},
    title = {Definitions},
    url = {https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence/part-1-the-basics-of-explaining-ai/definitions/},
    urldate = {2024-10-28},
    keywords = {bib}
}

@online{IBMChatbotDef,
    author = {IBM},
    title = {What is a chatbot?},
    url = {https://www.ibm.com/topics/chatbots},
    urldate = {2024-10-28},
    keywords = {bib}
}

@online{IBMGenAI,
    author = {IBM},
    title = {What is generative AI?},
    url = {https://research.ibm.com/blog/what-is-generative-AI},
    urldate = {2024-10-28},
    keywords = {bib}
}

@online{MITGenAI,
    author = {Adam Zewe},
    title = {Explained: Generative AI},
    date = {2023-11-09},
    url = {https://news.mit.edu/2023/explained-generative-ai-1109},
    urldate = {2024-10-28},
    keywords = {bib}
}

@online{CloudflareLLM,
    author = {Cloudflare},
    title = {What is a large language model (LLM)?},
    url = {https://www.cloudflare.com/en-gb/learning/ai/what-is-large-language-model/},
    urldate = {2024-10-28},
    keywords = {bib}
}

@online{UXDict,
    author = {{Cambridge Dictionary}},
    title = {Meaning of user experience in English},
    url = {https://dictionary.cambridge.org/dictionary/english/user-experience},
    urldate = {2024-10-28},
    keywords = {bib}
}

@online{IBMNLP,
	title = {What Is {NLP} (Natural Language Processing)? {\textbar} {IBM}},
	url = {https://www.ibm.com/topics/natural-language-processing},
	shorttitle = {What Is {NLP} (Natural Language Processing)?},
	abstract = {Natural language processing ({NLP}) is a subfield of artificial intelligence ({AI}) that uses machine learning to help computers communicate with human language.},
	author = {{IBM}},
	urldate = {2024-11-04},
	date = {2024-08-11},
	keywords = {bib}
}

