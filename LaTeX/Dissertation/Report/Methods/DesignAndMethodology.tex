\chapter{Methods and Implementation}
This chapter focuses on the experimental design and implementation of the artefact,
covering the self-imposed project management methodology, original concept design 
and the overall development process.

\section{Methodology}
When developing software, there are a wide variety of available options to manage the development 
process, which help to structure how time should be allocated as development progresses. 

\subsection{Waterfall} 
\para The first methodology considered was the Waterfall methodology, which is a very common 
approach to software development being sometimes referred to as the Software Development Life Cycle, or SDLC \autocite{adobePopularProjectManagement2023}.
Waterfall is a highly structured and strict methodology which enforces that one stage of development must be completed before the next can begin,
which creates a cascading set of steps, hence its namesake.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Implementation/Methodology/Waterfall.png}
    \caption{An overview of a Waterfall workflow \autocite{adobeWaterfallMethodologyProject}. \label{fig:Waterfall}}
\end{figure}

\noindent Waterfall begins by ascertaining all project requirements for all stages of the project, which 
would include costs, risks, associated dependencies and overall timelines for completions of each stage.
Following this is the design stage, where a general high-level design is created to demonstrate the 
project, and this design is then acted upon and implemented in the implementation stage. Then, the 
implementation is rigorously tested before its eventual deployment.

\para It is a methodology with a strong reputation due to its clear structure, with all necessary facts and figures 
being calculated in the requirements stage before any designs or development occur. The clear structure allows 
progress to be easily measured against each predefined milestone.

\para Though, despite these advantages, Waterfall brings with it some clear disadvantages - the first of which 
being that with all requirements being defined at the very beginning of the project's development,
it introduces significant difficulty should there be any further requirements specified during development. This would also 
bring in the second disadvantage known as 'deadline creep' \autocite{adobeWaterfallMethodologyProject}; 
if one stage is delayed, such as by request for additional features, this would then impact all subsequent stages. 

\subsection{Agile}
The second methodology considered was another highly reputed software development methodology known as Agile.
Unlike Waterfall which defines all stages and requirements at the beginning, Agile is a highly iterative methodology with steps
known as 'sprints' which are frequently repeated, providing a more incremental approach to development. Each of these sprints
would represent a small part of the program, eventually building up to the full version.

\para As depicted in Figure \ref{fig:ExampleAgileSprint}, 
Agile sprints begin by planning the overall aims of that particular sprint. Similarly to Waterfall, a high-level
design is then created and developed, before being rigorously tested. This is also one of Agile's key benefits; the 
constant testing of the small parts developed in each sprint helps ensure that all bugs can be rectified, unlike 
Waterfall where the whole product is tested and some smaller elements with bugs could potentially be overlooked.
After testing, the product of that sprint is deployed and reviewed. Then, the cycle begins anew with another 
sprint.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{Implementation/Methodology/Agile.png}
    \caption{An overview of an Agile sprint \autocite{asanaWhatAgileMethodology}\label{fig:ExampleAgileSprint}}
\end{figure}


\noindent The most prominent key benefit of Agile is its sprint-based iterative nature that allows for requirements to shift 
throughout development without major disruption. Furthermore, this incremental process minimises the risk of total project 
failure as usable components are constantly produced. In business environments, Agile also allows for enhanced teamwork, though 
this will not be present in this particular project.

\para As with Waterfall, Agile is not without drawbacks. Agile's most notable drawback is known as 'scope creep' \autocite{malsamWhatScopeCreep2024},
which occurs when requirements are continually added to a point where development can never truly end; the product continues to expand 
far beyond its original intentions to the point where maintenance becomes extremely difficult or outright impossible with an 
ever-expanding codebase. Furthermore, it is possible that because of this, the end product can be almost entirely different to its original concept.

\subsection{Comparison and decision}

Both methodologies bear strong benefits and drawbacks. The particular choice for this project is Agile, primarily because of the reduced 
risk through constant testing and also for its deeply flexible nature allowing the requirements of the project to potentially shift 
over time as needed, unlike Waterfall where this could cause major deadline creep. Additionally, the time-sensitive nature of this project 
best suits Agile's fast incremental sprints rather than the slower, more methodical Waterfall.

\section{Potential limitations and mitigations}\label{sec:Limitations}
The project as a whole bears some limitations of its own that may hinder the development process or the final product.

\subsection{Time}
The project is likely to take a considerable amount of time to develop to the excellent standard desired. This poses an issue 
in balancing time throughout the academic year alongside four other modules each with their own independent deadlines and workloads
of similar scale. As such, it is possible that if the product has issues, they could have been remedied with additional development 
time.

\para To mitigate this risk, a Gantt chart was developed to model the overall project timeline, and can be found in Appendix A.
% !!!!!!!!!!!!!!!! GANTT
% ! Didn't the proposal have risk assessments?

\subsection{Cost}
Due to the inability to use OpenAI's LLMs on a local device because of both their proprietary nature and extreme hardware requirements,
their public API will need to be used instead. This incurs a financial cost for every query sent and response received from the LLM,
dependent on the model chosen. For example, GPT-4o has a cost of \$2.50 per 1,000,000 input tokens \autocite{openaiPricingOpenAIAPI}.
Throughout the development and testing processes in each sprint, a cost will slowly begin to accrue.

\para Mitigating risks posed by potential cost constraints will be remedied by identifying the optimal model, which was decided to 
be gpt-4o-mini. The project has a limited budget due to it being a solo endeavour, though using a somewhat less intelligent model 
will greatly mitigate the risk of overspending while only compromising slightly on answer quality. 

\subsection{Experience}
Personally, I have never worked with LLM APIs before, nor the frameworks used to create apps with them such as LangChain. As such,
it is highly likely that many issues will be faced during the development process as I am forced to learn a tech stack that is completely 
new to me. This also links back to the previously mentioned time constraint, with the time taken to learn the modules used being time that 
could have been spent on development had I known them ahead of time.

\para To address this risk, time will be budgeted to allow for thorough research into the necessary tech stack to ensure that the project 
can be completed to a suitable standard.

\subsection{Independence}
This project is a solo venture with no support from others. As such, the previously mentioned issues of time and cost are entirely 
my own burden and responsibility. 
% ! Can anything really be said here other than 'I just dealt with it'?

\subsection{LLM Unpredictability}
LLMs are an extremely useful tool, being able to execute instructions given to them in natural language. However, without specific tuning, 
an LLM will not give the same response to the same prompt every time it is given. While this does add a sense of personality which could 
aid with a chatbot, it may risk answering questions incorrectly. This also can make LLM-based programs extremely challenging to debug due 
to this lack of reproducibility.

\para In an effort to mitigate any potential risk of LLM unpredictability, the chatbot's 'temperature' parameter will be set to 0, 
which will make its responses more static. This means that the chatbot should provide the same answer any time it is asked a certain 
question unless there are external circumstances (previous conversation history, etc.). 

\subsection{LangChain Documentation}
LangChain will be a critical element in this project's development, serving as the backend framework that the chatbot will run on.
Therefore, it is mandatory that I learn about it in order to produce a functional product, which would typically involve reading the 
documentation as is common when learning new modules. However, LangChain's documentation is frequently outdated and/or references 
functions or classes that have since been deprecated, without the documentation being updated. LangChain also frequently deprecates classes 
and functions with each new update, meaning that finding the current optimal methods for specific aims can be challenging.

\para Despite this risk, LangChain is recognised as a good framework for developing LLM-based applications and there are other 
resources that can be learned from that are not the documentation page, such as tutorial videos and blogs. Through using alternative 
resources alongside the official documentation, it should be possible to use LangChain to a good standard to produce an end product 
of suitable quality.

\section{Design}
Before any design concepts can be created, it is first necessary to establish what is being designed. Therefore, the functional and 
non-functional requirements for the chatbot were considered.

\subsection{Requirements}\label{sec:Requirements}
\subsubsection{Functional Requirements}
The following requirements are deemed essential to the chatbot's function, and the project cannot be considered complete unless they 
are fulfilled:

\begin{itemize}
    \item The chatbot must interpret and respond to answers in English.
    \item The chatbot must accept text queries.
    \item The chatbot must respond using text.
    \item The chatbot must be accessible at all times.
    \item The chatbot must supply BCU-related information.
    \item The chatbot must answer at least 75\% of BCU-related queries correctly.
    \item The chatbot must have a GUI for ease of use and accessibility.
    \item Multiple users must be able to use the chatbot at the same time.
\end{itemize}

\subsubsection{Non-functional Requirements}
The following requirements, while not essential, would be beneficial if fulfilled:

\begin{itemize}
    \item The chatbot should respond to queries within 10 seconds.
    \item The chatbot could allow for voice input and output.
    \item The chatbot could be deployed on an existing messaging service such as Teams.
\end{itemize}

\subsection{Concept diagrams}

Figure \ref{fig:ConceptDiagram} depicts a theoretical interaction with the chatbot from the frontend user's perspective created 
early in development.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Implementation/ConceptDiagram.png}
    \caption{An early visualisation of a conversation.\label{fig:ConceptDiagram}}
\end{figure}

\noindent Users will have a clearly labelled text input box, and a messaging interface similar to other text messaging apps 
which they would hopefully be familiar with allowing for them to quickly understand how to interact with the chatbot.
This version of the chatbot would be hosted locally, though with additional time and resources it could instead be hosted on a 
dedicated domain or as part of another service.

\para Figures \ref{fig:EmbedFlowchart} and \ref{fig:RAGFlowchart} depict how the backend of the chatbot should work, including the 
storage of BCU data and how the chatbot will query it.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Implementation/LucidEmbedFlow.png}
    \caption{The flowchart for the PDF embedding procedure.\label{fig:EmbedFlowchart}}
\end{figure}

\noindent BCU's policies are publically available on their website \autocite{bcuPoliciesProcedures}, with policies valid during the 2024/25 academic
year being used in this project. The planned use case for these policies was to download them locally and then process them using LangChain as a 
wrapper to split the PDFs into smaller chunks and embed them into a vector database. LangChain provides classes for direct PDF loading and 
conversion into its own "Document" format, and also provides functions to vectorise each chunk using any embedding model, with OpenAI's 
text-embedding-3-small being used. 

\para After the embedded chunks are stored, the database generated from this process is stored locally. This means that the chatbot will operate 
both locally and on the cloud, with vector similarity searches being executed on the chatbot host device and the proprietary gpt-4o-mini LLM running 
on an OpenAI server.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Implementation/LucidRAGFlow.png}
    \caption{The flowchart for the answer generation procedure.\label{fig:RAGFlowchart}}
\end{figure}

\noindent Users will input prompts as natural language to the chatbot, which will then vectorise them using the same embedding model used 
to embed the BCU policies. This is a necessary procedure as different embedding models will yield different outputs, meaning that if the same 
model was not used for the storage of policies and conversion of user queries, answers would be completely incorrect. 

\para In the interest of saving costs and reducing response times, the chatbot will ideally not query its university information vector store 
unless it cannot answer a question without it. This is because appending the university information, even in small amounts, would 
greatly increase the token usage of each individual prompt. To do so, the chatbot will review the existing conversation when it receives a 
prompt. If it already has the answer to the user's question in previously retrieved context, there will be no need to perform another search 
and duplicate pre-existing context.

\para Another way that the chatbot may choose not to query the database is if the user's prompt can already be answered by the LLM. This may 
occur with very simple queries, such as "Hello!" or "My name is Lewis." In either of these example scenarios, the chatbot should not query the 
database, as the LLM should be capable of responding to generic greetings and remembering a user's name. Additionally, answers that do not 
require similarity searches should be answered considerably quicker, and would make the user experience feel much smoother if they are not 
having to wait a long time between responses as previously detailed in Section \ref{sec:Requirements}. 

\para This decision functionality would be provided by LangGraph, and is detailed 
further in Section \ref{sec:ChatbotBackend}.



